{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400bcfa6-e896-42cd-8a82-2a1bfd7911ad",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab2910a-2928-4e74-b914-a2e63ce0302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh hasil pra-pemrosesan (setelah penanganan 'nya' terpisah):\n",
      "Teks Asli: game nya bagus tapi gacha nya ampas\n",
      "Setelah Digabung: gamenya bagus tapi gachanya ampas\n",
      "Hasil Akhir Clean: game bagus gacha ampas\n",
      "\n",
      "--- Hasil pada DataFrame ---\n",
      "                                             content  \\\n",
      "0  udah warp 140 kali masih blum dapat B5 yg ada ...   \n",
      "1  Bagus game hoyo versi turn base, banyak bansos...   \n",
      "2      bagus, tapi semenjak update besar bngt GB nya   \n",
      "3        global passive? HP inflation? pfft* hell no   \n",
      "4  game nyah sudah saya hapus susah gacha nyah ap...   \n",
      "\n",
      "                                     content_cleaned  \n",
      "0  warp kali b baner clara warp sisa b mulu susah...  \n",
      "1  bagus game hoyo versi turn base bansos kekuran...  \n",
      "2                    bagus semenjak update banget gb  \n",
      "3              global passive hp inflation pfft hell  \n",
      "4  game  hapus susah gacha  free play nyari  ngum...  \n",
      "\n",
      "Jumlah data setelah difilter: 14502\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Download NLTK stopwords jika belum ada ---\n",
    "try:\n",
    "    stopwords.words('indonesian')\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# --- 1. Memuat Data ---\n",
    "df = pd.read_csv('scrapped_data_honkai_star_rail.csv')\n",
    "\n",
    "# --- 2. Case Folding ---\n",
    "df['content_cleaned'] = df['content'].str.lower()\n",
    "\n",
    "# --- 3. Menghilangkan Noise ---\n",
    "def remove_noise(text):\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text\n",
    "df['content_cleaned'] = df['content_cleaned'].apply(remove_noise)\n",
    "\n",
    "# --- 4. BARU: Menggabungkan 'nya' yang Terpisah ---\n",
    "def rejoin_nya(text):\n",
    "    # Menggunakan regex untuk menemukan pola \"kata spasi nya\"\n",
    "    # \\b memastikan kita hanya mencocokkan kata 'nya' yang berdiri sendiri\n",
    "    return re.sub(r'(\\w+)\\s+nya\\b', r'\\1nya', text)\n",
    "df['content_cleaned'] = df['content_cleaned'].apply(rejoin_nya)\n",
    "\n",
    "\n",
    "# --- 5. Normalisasi Kata (Singkatan) ---\n",
    "slang_dict = {\n",
    "    'yg': 'yang', 'tpi': 'tapi', 'bg': 'bang', 'sy': 'saya', 'ga': 'tidak', 'gak': 'tidak',\n",
    "    'gk': 'tidak', 'gaada': 'tidak ada', 'dpt': 'dapat', 'bgt': 'banget', 'bngt': 'banget',\n",
    "    'lg': 'lagi', 'gw': 'saya', 'gue': 'saya', 'lu': 'kamu', 'udh': 'sudah', 'udah': 'sudah',\n",
    "    'kalo': 'kalau', 'aja': 'saja', 'kyk': 'seperti', 'jg': 'juga', 'blm': 'belum',\n",
    "    'gmn': 'bagaimana', 'knp': 'kenapa', 'trs': 'terus', 'sm': 'sama', 'cmn': 'cuma','blum': 'belum', \n",
    "    'nyah': 'nya', 'lagih': 'lagi', 'dapet': 'dapat', 'gem': 'game'\n",
    "}\n",
    "def normalize_slang(text):\n",
    "    words = text.split()\n",
    "    normalized_words = [slang_dict[word] if word in slang_dict else word for word in words]\n",
    "    return ' '.join(normalized_words)\n",
    "df['content_cleaned'] = df['content_cleaned'].apply(normalize_slang)\n",
    "\n",
    "# --- 6. Stopword Removal (Gabungan & Penanganan 'nya' yang sudah digabung) ---\n",
    "stop_words_id = set(stopwords.words('indonesian'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "combined_stopwords = stop_words_id.union(stop_words_en)\n",
    "\n",
    "def enhanced_stopword_removal(text):\n",
    "    words = text.split()\n",
    "    cleaned_words = [word[:-3] if word.endswith('nya') else word for word in words]\n",
    "    filtered_words = [word for word in cleaned_words if word not in combined_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "df['content_cleaned'] = df['content_cleaned'].apply(enhanced_stopword_removal)\n",
    "\n",
    "# --- 7. Filtrasi Kalimat Tidak Bermakna ---\n",
    "df_filtered = df[df['content_cleaned'].apply(lambda x: len(x.split()) > 3)]\n",
    "\n",
    "# Tampilkan perbandingan hasilnya\n",
    "print(\"Contoh hasil pra-pemrosesan (setelah penanganan 'nya' terpisah):\")\n",
    "# Contoh manual untuk menunjukkan cara kerja rejoin_nya\n",
    "contoh_teks = \"game nya bagus tapi gacha nya ampas\"\n",
    "print(f\"Teks Asli: {contoh_teks}\")\n",
    "print(f\"Setelah Digabung: {rejoin_nya(contoh_teks)}\")\n",
    "print(f\"Hasil Akhir Clean: {enhanced_stopword_removal(normalize_slang(rejoin_nya(contoh_teks)))}\")\n",
    "\n",
    "print(\"\\n--- Hasil pada DataFrame ---\")\n",
    "print(df_filtered[['content', 'content_cleaned']].head())\n",
    "print(f\"\\nJumlah data setelah difilter: {len(df_filtered)}\")\n",
    "\n",
    "# --- Data SIAP untuk BERTopic ---\n",
    "docs = df_filtered['content_cleaned'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa83ae5-a135-4416-91f3-10299464c589",
   "metadata": {},
   "source": [
    "## INDOBERT DAN BERTOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4499bb-cd5e-41e8-9fa0-babbbaad5f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1016\\1445959073.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# --- 1. Siapkan Data untuk Model ---\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Mengambil daftar ulasan yang sudah bersih\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- 1. Siapkan Data untuk Model ---\n",
    "# Mengambil daftar ulasan yang sudah bersih\n",
    "docs = df_filtered['content_cleaned'].tolist()\n",
    "\n",
    "# --- 2. Muat Model IndoBERT ---\n",
    "# Kita menggunakan model pre-trained yang sudah dilatih untuk Bahasa Indonesia\n",
    "embedding_model = SentenceTransformer('indobenchmark/indobert-base-p1')\n",
    "\n",
    "\n",
    "# --- 3. Inisialisasi dan Latih Model BERTopic ---\n",
    "# nr_topics=\"auto\" akan secara otomatis menentukan jumlah topik yang optimal\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\",\n",
    "    min_topic_size=50 # Topik minimal harus memiliki 50 anggota/ulasan\n",
    ")\n",
    "\n",
    "# Latih model (ini mungkin akan memakan waktu)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a025f-442c-4a11-8ee1-d21252c0d754",
   "metadata": {},
   "source": [
    "## Evaluasi Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "779c8fda-abdf-4126-896f-9562195e5e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skor Koherensi Topik (C_v): 0.5607861132342764\n"
     ]
    }
   ],
   "source": [
    "# Import library tambahan untuk evaluasi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import numpy as np\n",
    "\n",
    "# --- BAGIAN BARU: EVALUASI KOHERENSI TOPIK ---\n",
    "\n",
    "# 1. Ekstraksi Topik dan Dokumen\n",
    "# Dapatkan representasi topik dari BERTopic\n",
    "topics_representation = topic_model.get_topics()\n",
    "\n",
    "# Filter out outlier topic (Topic -1)\n",
    "topics_to_eval = {k: v for k, v in topics_representation.items() if k != -1}\n",
    "# Urutkan berdasarkan ID topik\n",
    "sorted_topics = [topics_to_eval[key] for key in sorted(topics_to_eval.keys())]\n",
    "# Ekstrak hanya kata-kata kunci dari setiap topik\n",
    "topic_words = [[word for word, _ in topic] for topic in sorted_topics]\n",
    "\n",
    "\n",
    "# 2. Tokenisasi Dokumen\n",
    "# Proses ini penting karena CoherenceModel bekerja dengan token (kata-kata)\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(docs) # Pelajari vocabulary dari seluruh dokumen\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "tokenized_docs = [tokenizer(doc) for doc in docs]\n",
    "\n",
    "\n",
    "# 3. Buat Dictionary dan Corpus untuk Gensim\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "\n",
    "# 4. Hitung Skor Koherensi C_v\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=tokenized_docs,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"\\nSkor Koherensi Topik (C_v): {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a78890-30ec-41fc-88ce-ad95d4194ad3",
   "metadata": {},
   "source": [
    "## Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e05027-0f4a-4190-98e4-7396de5c94ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# a. Visualisasi Jarak Antar Topik\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m fig1 = \u001b[43mtopic_model\u001b[49m.visualize_topics()\n\u001b[32m      3\u001b[39m fig1.show()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# b. Visualisasi Kata Kunci per Topik\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# a. Visualisasi Jarak Antar Topik\n",
    "fig1 = topic_model.visualize_topics()\n",
    "fig1.show()\n",
    "\n",
    "# b. Visualisasi Kata Kunci per Topik\n",
    "fig2 = topic_model.visualize_barchart(top_n_topics=10) # 10 topik teratas\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cccee6-c6a9-4174-b57d-eccd282955a3",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a628bde1-e6d6-42c0-90ab-e280e73689c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memulai Parameter Tuning (dengan nilai lebih rendah) ---\n",
      "\n",
      "Menguji dengan min_topic_size = 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Topik Ditemukan: 2\n",
      "Skor Koherensi (C_v): 0.6091\n",
      ">>> Skor terbaik baru ditemukan!\n",
      "\n",
      "Menguji dengan min_topic_size = 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Topik Ditemukan: 4\n",
      "Skor Koherensi (C_v): 0.5547\n",
      "\n",
      "Menguji dengan min_topic_size = 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Topik Ditemukan: 5\n",
      "Skor Koherensi (C_v): 0.6006\n",
      "\n",
      "Menguji dengan min_topic_size = 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Topik Ditemukan: 4\n",
      "Skor Koherensi (C_v): 0.5547\n",
      "\n",
      "--- Tuning Selesai ---\n",
      "Parameter terbaik: min_topic_size = 20\n",
      "Skor Koherensi (C_v) tertinggi: 0.6091\n",
      "\n",
      "Membuat visualisasi dari model terbaik...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Setelah menemukan model terbaik, visualisasi akan berhasil\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMembuat visualisasi dari model terbaik...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fig = \u001b[43mbest_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     fig.show()\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\bertopic\\_bertopic.py:2240\u001b[39m, in \u001b[36mBERTopic.visualize_topics\u001b[39m\u001b[34m(self, topics, top_n_topics, custom_labels, title, width, height)\u001b[39m\n\u001b[32m   2207\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" Visualize topics, their sizes, and their corresponding words\u001b[39;00m\n\u001b[32m   2208\u001b[39m \n\u001b[32m   2209\u001b[39m \u001b[33;03mThis visualization is highly inspired by LDAvis, a great visualization\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2237\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m   2238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2239\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplotting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_n_topics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\bertopic\\plotting\\_topics.py:79\u001b[39m, in \u001b[36mvisualize_topics\u001b[39m\u001b[34m(topic_model, topics, top_n_topics, custom_labels, title, width, height)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m topic_model.topic_embeddings_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     embeddings = topic_model.topic_embeddings_[indices]\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     embeddings = \u001b[43mUMAP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcosine\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m     embeddings = topic_model.c_tf_idf_.toarray()[indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:2935\u001b[39m, in \u001b[36mUMAP.fit_transform\u001b[39m\u001b[34m(self, X, y, ensure_all_finite, **kwargs)\u001b[39m\n\u001b[32m   2897\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, ensure_all_finite=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m   2898\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[32m   2899\u001b[39m \u001b[33;03m    output.\u001b[39;00m\n\u001b[32m   2900\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2933\u001b[39m \u001b[33;03m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[32m   2934\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2935\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2936\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform_mode == \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2937\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_dens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:2817\u001b[39m, in \u001b[36mUMAP.fit\u001b[39m\u001b[34m(self, X, y, ensure_all_finite, **kwargs)\u001b[39m\n\u001b[32m   2813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform_mode == \u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2814\u001b[39m     epochs = (\n\u001b[32m   2815\u001b[39m         \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs\n\u001b[32m   2816\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2817\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_, aux_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_embed_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# JH why raw data?\u001b[39;49;00m\n\u001b[32m   2822\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2825\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_epochs_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2826\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33membedding_list\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m aux_data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:2872\u001b[39m, in \u001b[36mUMAP._fit_embed_data\u001b[39m\u001b[34m(self, X, n_epochs, init, random_state, **kwargs)\u001b[39m\n\u001b[32m   2867\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_fit_embed_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, n_epochs, init, random_state, **kwargs):\n\u001b[32m   2868\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[33;03m    replaced by subclasses. Arbitrary keyword arguments can be passed\u001b[39;00m\n\u001b[32m   2870\u001b[39m \u001b[33;03m    through .fit() and .fit_transform().\u001b[39;00m\n\u001b[32m   2871\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimplicial_set_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initial_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2878\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2879\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepulsion_strength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2880\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnegative_sample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2882\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2884\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2885\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2886\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdensmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2887\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_densmap_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2889\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_distance_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2890\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_metric_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2891\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meuclidean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2892\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2893\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2894\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtqdm_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2895\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\umap\\umap_.py:1089\u001b[39m, in \u001b[36msimplicial_set_embedding\u001b[39m\u001b[34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[39m\n\u001b[32m   1086\u001b[39m n_epochs_max = \u001b[38;5;28mmax\u001b[39m(n_epochs) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_epochs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n_epochs\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_epochs_max > \u001b[32m10\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     graph.data[graph.data < (\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / \u001b[38;5;28mfloat\u001b[39m(n_epochs_max))] = \u001b[32m0.0\u001b[39m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1091\u001b[39m     graph.data[graph.data < (graph.data.max() / \u001b[38;5;28mfloat\u001b[39m(default_epochs))] = \u001b[32m0.0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_methods.py:41\u001b[39m, in \u001b[36m_amax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_amax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     40\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# Import library tambahan yang dibutuhkan\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUSI FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Buat model HDBSCAN kustom dengan setelan yang lebih ketat\n",
    "# Coba naikkan `min_samples`. Semakin tinggi nilainya, semakin banyak data yang akan dianggap outlier.\n",
    "# Nilai `min_cluster_size` harus sama dengan `min_topic_size` yang ingin Anda uji.\n",
    "hdbscan_model_tuned = HDBSCAN(\n",
    "    min_cluster_size=15,  # Sesuaikan dengan min_topic_size yang ingin Anda uji\n",
    "    min_samples=25,       # NAIKKAN NILAI INI. Coba dengan 15, 25, atau 35.\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# (Opsional) Anda juga bisa mengontrol UMAP jika perlu\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "\n",
    "# 2. Latih kembali BERTopic, tapi kali ini berikan model HDBSCAN kustom kita\n",
    "# Model akan menggunakan setelan HDBSCAN kita, bukan setelan default-nya lagi.\n",
    "topic_model_tuned = BERTopic(\n",
    "    embedding_model='indobenchmark/indobert-base-p2', # Gunakan p2 untuk hasil yang lebih baik\n",
    "    min_topic_size=15,                                # Sesuaikan dengan min_cluster_size di atas\n",
    "    hdbscan_model=hdbscan_model_tuned,                # INI BAGIAN PENTINGNYA\n",
    "    umap_model=umap_model,                            # Opsional\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Latih model yang sudah di-tuning dengan data Anda ('docs')\n",
    "topics, probs = topic_model_tuned.fit_transform(docs)\n",
    "\n",
    "# 3. Lihat hasilnya\n",
    "print(\"\\n--- Hasil Setelah Fine-Tuning HDBSCAN ---\")\n",
    "print(topic_model_tuned.get_topic_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f9bccb-f1eb-45a1-9fcd-7667137b10c3",
   "metadata": {},
   "source": [
    "## Visualisasi setelah Fine Tuning Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a16e66-4f31-4d69-9d60-03abedd0fe5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbest_model\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mParameter terbaik ditemukan: min_topic_size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_min_topic_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkor Koherensi (C_v) tertinggi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "if best_model:\n",
    "    print(f\"\\nParameter terbaik ditemukan: min_topic_size = {best_min_topic_size}\")\n",
    "    print(f\"Skor Koherensi (C_v) tertinggi: {best_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n>>> Menampilkan Tabel Informasi Topik dari Model Terbaik:\")\n",
    "    print(best_model.get_topic_info())\n",
    "    \n",
    "    print(\"\\n>>> Membuat Visualisasi dari Model Terbaik...\")\n",
    "    # Visualisasi Jarak Antar Topik\n",
    "    fig1 = best_model.visualize_topics()\n",
    "    fig1.show()\n",
    "    \n",
    "    # Visualisasi Kata Kunci per Topik\n",
    "    fig2 = best_model.visualize_barchart(top_n_topics=10)\n",
    "    fig2.show()\n",
    "else:\n",
    "    print(\"\\nTidak ada kombinasi parameter yang menghasilkan cukup topik.\")\n",
    "    print(\"Saran: Coba turunkan lagi nilai dalam 'min_topic_sizes_to_test' (misal: [10, 15, 20]).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8eb304-c9e0-4b38-bbae-dcb9f6eadd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

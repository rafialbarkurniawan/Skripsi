{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6740dee-1773-44d0-99ca-5139dc45b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Nama kolom dataset asli:\n",
      "['userName', 'score', 'at', 'content']\n",
      "Jumlah total data asli: 25413\n",
      "âœ… Kolom ulasan terdeteksi: 'content'\n",
      "â„¹ï¸ Tidak ada kolom 'lang' atau 'sentiment'. Sampling dilakukan secara acak.\n",
      "\n",
      "ðŸ“Š Membuat dummy 5% (1270 data)...\n",
      "âœ… Dummy 5% selesai dibuat: 1270 baris.\n",
      "ðŸ’¾ File disimpan ke: ./honkai_dummy_5pct.csv\n",
      "\n",
      "ðŸ“Š Membuat dummy 10% (2541 data)...\n",
      "âœ… Dummy 10% selesai dibuat: 2541 baris.\n",
      "ðŸ’¾ File disimpan ke: ./honkai_dummy_10pct.csv\n",
      "\n",
      "ðŸ“Š Membuat dummy 25% (6353 data)...\n",
      "âœ… Dummy 25% selesai dibuat: 6353 baris.\n",
      "ðŸ’¾ File disimpan ke: ./honkai_dummy_25pct.csv\n",
      "\n",
      "ðŸŽ¯ Semua file dummy berhasil dibuat dan disimpan:\n",
      "- 5% â†’ ./honkai_dummy_5pct.csv\n",
      "- 10% â†’ ./honkai_dummy_10pct.csv\n",
      "- 25% â†’ ./honkai_dummy_25pct.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# =====================================================\n",
    "# 1. PEMANGGILAN DATA ASLI\n",
    "# =====================================================\n",
    "df_real = pd.read_csv(\"scrapped_data_honkai_star_rail.csv\")\n",
    "\n",
    "print(\"ðŸ“„ Nama kolom dataset asli:\")\n",
    "print(df_real.columns.tolist())\n",
    "print(f\"Jumlah total data asli: {len(df_real)}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. DETEKSI KOLOM ULASAN UTAMA\n",
    "# =====================================================\n",
    "TEXT_COL = None\n",
    "for candidate in ['clean_final', 'review', 'content', 'comment', 'text', 'body', 'reviewBody']:\n",
    "    if candidate in df_real.columns:\n",
    "        TEXT_COL = candidate\n",
    "        break\n",
    "\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(\"âŒ Kolom teks ulasan tidak ditemukan. Tentukan kolom teks utama secara manual.\")\n",
    "\n",
    "print(f\"âœ… Kolom ulasan terdeteksi: '{TEXT_COL}'\")\n",
    "\n",
    "# Hapus data kosong atau hanya berisi spasi\n",
    "df_real = df_real.dropna(subset=[TEXT_COL])\n",
    "df_real = df_real[df_real[TEXT_COL].astype(str).str.strip() != \"\"]\n",
    "\n",
    "# =====================================================\n",
    "# 3. IDENTIFIKASI KOLOM UNTUK STRATIFIKASI (JIKA ADA)\n",
    "# =====================================================\n",
    "possible_strata = [c for c in ['lang', 'sentiment'] if c in df_real.columns]\n",
    "if len(possible_strata) > 0:\n",
    "    print(f\"ðŸ”¹ Kolom stratifikasi digunakan: {possible_strata}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Tidak ada kolom 'lang' atau 'sentiment'. Sampling dilakukan secara acak.\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. FUNGSI PEMBUAT DATA DUMMY\n",
    "# =====================================================\n",
    "def make_dummy(df, pct=0.05, stratify_cols=None, text_col='review', random_state=42):\n",
    "    \"\"\"\n",
    "    Membuat subset data dummy berdasarkan persentase tertentu.\n",
    "    Jika kolom stratifikasi tersedia, sampling dilakukan secara proporsional.\n",
    "    Augmentasi ringan ditambahkan untuk menambah variasi teks.\n",
    "    \"\"\"\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n_target = max(1, int(len(df) * pct))\n",
    "    print(f\"\\nðŸ“Š Membuat dummy {pct*100:.0f}% ({n_target} data)...\")\n",
    "\n",
    "    # Sampling proporsional jika kolom stratifikasi ada\n",
    "    if stratify_cols and all(c in df.columns for c in stratify_cols):\n",
    "        df_temp = df.copy()\n",
    "        df_temp['__key__'] = df_temp[stratify_cols].astype(str).agg('_'.join, axis=1)\n",
    "        df_sample = (\n",
    "            df_temp.groupby('__key__', group_keys=False)\n",
    "            .apply(lambda x: x.sample(frac=pct, replace=len(x)*pct > len(x), random_state=random_state))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        df_sample.drop(columns=['__key__'], inplace=True, errors='ignore')\n",
    "    else:\n",
    "        df_sample = df.sample(n=n_target, random_state=random_state)\n",
    "\n",
    "    # Tambahkan kolom default bila belum ada\n",
    "    if 'lang' not in df_sample.columns:\n",
    "        df_sample['lang'] = 'unknown'\n",
    "    if 'sentiment' not in df_sample.columns:\n",
    "        df_sample['sentiment'] = 'unknown'\n",
    "\n",
    "    # Augmentasi ringan teks (menambah kata acak 20% peluang)\n",
    "    def augment_text(t):\n",
    "        t = str(t)\n",
    "        if random.random() < 0.2:\n",
    "            t += \" \" + random.choice([\"gacha\", \"pity\", \"banner\", \"turn-based\", \"update\", \"lag\"])\n",
    "        return t\n",
    "\n",
    "    df_sample[text_col] = df_sample[text_col].apply(augment_text)\n",
    "\n",
    "    print(f\"âœ… Dummy {pct*100:.0f}% selesai dibuat: {len(df_sample)} baris.\")\n",
    "    return df_sample\n",
    "\n",
    "# =====================================================\n",
    "# 5. PEMBUATAN DAN PENYIMPANAN DATA DUMMY\n",
    "# =====================================================\n",
    "output_dir = \"./\"\n",
    "sizes = [0.05, 0.10, 0.25]  # 5%, 10%, dan 25% dari total data\n",
    "dummies = {}\n",
    "\n",
    "for pct in sizes:\n",
    "    dummy_df = make_dummy(\n",
    "        df_real,\n",
    "        pct=pct,\n",
    "        stratify_cols=possible_strata,\n",
    "        text_col=TEXT_COL\n",
    "    )\n",
    "    out_path = os.path.join(output_dir, f\"honkai_dummy_{int(pct*100)}pct.csv\")\n",
    "    dummy_df.to_csv(out_path, index=False, encoding='utf-8')\n",
    "    dummies[pct] = out_path\n",
    "    print(f\"ðŸ’¾ File disimpan ke: {out_path}\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. RANGKUMAN\n",
    "# =====================================================\n",
    "print(\"\\nðŸŽ¯ Semua file dummy berhasil dibuat dan disimpan:\")\n",
    "for k, v in dummies.items():\n",
    "    print(f\"- {int(k*100)}% â†’ {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cf4714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# CELL 2: FUNGSI PREPROCESS UNTUK 1 FILE\n",
    "# ==============================================\n",
    "def preprocess_file(input_csv: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Menerapkan pipeline preprocessing (case folding, cleaning,\n",
    "    normalisasi, tokenizing, stopword removal, stemming)\n",
    "    ke satu file CSV dan menyimpan hasil ke output_csv.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== PREPROCESSING: {input_csv} ===\")\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"File tidak ditemukan: {input_csv}\")\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Deteksi kolom teks utama\n",
    "    possible_text_cols = ['review', 'content', 'comment', 'text', 'body', 'reviewBody']\n",
    "    TEXT_COL = next((c for c in possible_text_cols if c in df.columns), None)\n",
    "    if TEXT_COL is None:\n",
    "        raise ValueError(f\"Tidak ditemukan kolom teks utama di {input_csv}. \"\n",
    "                         f\"Kolom yang dicek: {possible_text_cols}\")\n",
    "    \n",
    "    # Ambil hanya 1 kolom teks dan buang NaN\n",
    "    df = df[[TEXT_COL]].rename(columns={TEXT_COL: 'content'}).dropna().reset_index(drop=True)\n",
    "    print(f\"  Data dimuat: {len(df)} baris dari kolom '{TEXT_COL}' -> 'content'\")\n",
    "    \n",
    "    # Jalankan pipeline bertahap\n",
    "    df[\"case_folding\"] = df[\"content\"].apply(case_folding)\n",
    "    df[\"cleaned\"] = df[\"case_folding\"].apply(clean_text)\n",
    "    df[\"normalized\"] = df[\"cleaned\"].apply(normalize_text)\n",
    "    df[\"token\"] = df[\"normalized\"].apply(tokenize_text)\n",
    "    df[\"no_stopword\"] = df[\"token\"].apply(remove_stopwords)\n",
    "    df[\"stemmed\"] = df[\"no_stopword\"].apply(stem_tokens)\n",
    "    df[\"clean_final\"] = df[\"stemmed\"].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    # Filter dokumen benar-benar kosong setelah preprocessing\n",
    "    before = len(df)\n",
    "    df[\"clean_final_length\"] = df[\"clean_final\"].str.split().apply(lambda x: len(x) if isinstance(x, list) else len(str(x).split()))\n",
    "    df = df[df[\"clean_final\"].str.strip() != \"\"].copy()\n",
    "    after = len(df)\n",
    "    df = df.drop(columns=[\"clean_final_length\"])\n",
    "    \n",
    "    print(f\"  Setelah filter kosong: {after} dokumen tersisa (buang {before - after})\")\n",
    "    \n",
    "    # Simpan hasil (minimal kolom yang nanti dipakai BERTopic: content, clean_final)\n",
    "    out_cols = [\"content\", \"clean_final\"]\n",
    "    df[out_cols].to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"  Hasil disimpan ke: {output_csv}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb53861-3b1b-49a6-a483-c0a19373013f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# CELL 1: SETUP PREPROCESSING\n",
    "# ==============================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from typing import List\n",
    "\n",
    "# Download resource NLTK (jalan sekali saja)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Normalisasi kata tidak baku (versi final)\n",
    "# ----------------------------------------------\n",
    "normalization_dict = {\n",
    "    \"ga\": \"tidak\", \"gak\": \"tidak\", \"gaada\": \"tidak ada\", \"gacha\": \"gacha\", \"pity\": \"pity\", \n",
    "    \"banner\": \"banner\",\"bgt\": \"sekali\", \"aja\": \"saja\", \"jd\": \"jadi\", \"yg\": \"yang\", \"tpi\": \"tapi\", \n",
    "    \"bg\": \"bang\", \"sy\": \"saya\",\"gk\": \"tidak\", \"dpt\": \"dapat\", \"bngt\": \"sekali\", \"lg\": \"lagi\",\n",
    "    \"lgi\":\"lagi\", \"gw\": \"saya\", \"gue\": \"saya\",\"lu\": \"kamu\",\"udh\": \"sudah\",\"udah\": \"sudah\",\"mksih\": \"terima kasih\", \n",
    "    \"kalo\": \"kalau\", \"kyk\": \"seperti\", \"jg\": \"juga\", \"blm\": \"belum\", \"gmn\": \"bagaimana\", \"kek\": \"seperti\",\n",
    "    \"knp\": \"kenapa\", \"trs\": \"terus\", \"sm\": \"sama\", \"cmn\": \"cuma\",\"blum\": \"belum\", \"nyah\": \"nya\",\n",
    "    \"lagih\": \"lagi\",\"dapet\": \"dapat\", \"gem\": \"game\", \"relic\": \"relic\", \"bnyk\": \"banyak\", \n",
    "    \"bgtt\": \"sekali\", \"baiq\": \"baik\",\"geme\": \"game\",\"gim\": \"game\", \"heba\": \"hebat\", \"rame\": \"ramai\",\n",
    "    \"mantep\": \"mantap\", \"muas\": \"puas\", \"pls\": \"please\", \"ny\": \"nya\", \"nitip\": \"menitip\", \"gakan\": \"tidak akan\",\n",
    "    \"pw\": \"power\", \"tlonk\": \"tolong\", \"mw\": \"mau\", \"smoga\": \"semoga\", \"sma\": \"sama\", \"kayak\": \"seperti\",\n",
    "    \"dlu\": \"dulu\", \"bgtu\": \"begitu\",\"tros\": \"terus\", \"plis\": \"please\", \"gamenya\": \"game nya\", \n",
    "    \"makasih\": \"terima kasih\", \"blom\": \"belum\",\"seru banget\": \"sangat seru\", \"seru bgt\": \"sangat seru\",\n",
    "    \"mantap bgt\": \"sangat mantap\",\"smua\": \"semua\",\"adain\": \"adakan\", \"adainya\": \"adakan nya\", \n",
    "    \"dgn\": \"dengan\", \"dlm\": \"dalam\", \"hrus\": \"harus\", \"jgn\": \"jangan\",\"jga\": \"juga\", \"jdi\": \"jadi\", \n",
    "    \"jgnlah\": \"janganlah\", \"kpn\": \"kapan\", \"krn\": \"karena\", \"lbh\": \"lebih\",\"mksd\": \"maksud\", \n",
    "    \"msti\": \"mesti\", \"nnti\": \"nanti\", \"sblm\": \"sebelum\", \"sbg\": \"sebagai\", \"sblmnya\": \"sebelum nya\",\n",
    "    \"smg\": \"semoga\", \"spt\": \"seperti\", \"syr\": \"saya\", \"syg\": \"sayang\", \"tkt\": \"takut\", \"trus\": \"terus\", \n",
    "    \"tlong\":\"tolong\", \"u/\": \"untuk\", \"utk\": \"untuk\", \"gamplay\": \"game play\", \"gameplay\": \"game play\", \n",
    "    \"banget\": \"sekali\",\"gemnya\": \"game nya\", \"nolongin\": \"menolong\", \"nolong\": \"menolong\", \n",
    "    \"tolongin\": \"menolong\", \"kg\":\"tidak\", \"kgk\": \"tidak\",\"kga\":\"tidak\",\"kagak\":\"tidak\",\"kaga\":\"tidak\",\n",
    "    \"kagk\":\"tidak\",\"ngak\":\"tidak\",\"ngga\":\"tidak\",\"nggak\":\"tidak\",\"ngk\":\"tidak\",\"tdk\":\"tidak\",\n",
    "    \"tdak\":\"tidak\", \"trimakasih\":\"terima kasih\", \"trmksh\":\"terima kasih\", \"trmksih\":\"terima kasih\",\n",
    "    \"ngerilis\":\"merilis\",\"gg\":\"good game\", \"geming\": \"game\",\"gemink\":\"game\",\"gamink\":\"game\",\n",
    "    \"hongkai\":\"honkai\", \"seruh\":\"seru\",\"maksih\":\"terima kasih\",\"dari\":\"dari\", \"bosen\":\"bosan\",\n",
    "    \"bacot\":\"bicara\",\"bacotin\":\"membicarakan\",\"bacotinlah\":\"membicarakanlah\", \"ngotak\":\"berpikir\",\n",
    "    \"ngasih\":\"memberi\", \"ngambil\":\"mengambil\", \"ngeluarin\":\"mengeluarkan\",\"ngebug\":\"terjadi bug\", \n",
    "    \"ngelepasin\":\"melepaskan\", \"ngecas\":\"mengisi\",\"ngebantu\":\"membantu\",\"mntp\":\"mantap\",\"ngefix\":\"memperbaiki\",\n",
    "    \"ngeban\":\"memblokir\",\"ngeblok\":\"memblokir\",\"ngebanned\":\"diblokir\",\"banned\":\"diblokir\", \"nunggu\":\"menunggu\", \n",
    "    \"ngumpulin\":\"mengumpulkan\",\"bnget\":\"sekali\", \"bgtu\":\"begitu\",\"snggat\":\"sangat\",\"sngt\": \"sangat\",\n",
    "    \"sbenernya\":\"sebenarnya\",\"sebenernya\":\"sebenarnya\",\"bgd\":\"banget\", \"bgs\":\"bagus\", \"kurng\":\"kurang\",\n",
    "    \"krng\":\"kurang\",\"krang\":\"kurang\",\"slalu\":\"selalu\", \"smakin\":\"semakin\", \"syaratnya\":\"syarat nya\", \n",
    "    \"ketentuanya\":\"ketentuan nya\", \"app\": \"aplikasi\",\"apani\": \"apa ini\", \"apaan\":\"apa\", \"apanya\":\"apa nya\", \n",
    "    \"ntar\":\"nanti\", \"ntr\":\"nanti\",\"mager\":\"malas gerak\", \"nyerang\":\"menyerang\",\"sya\": \"saya\", \n",
    "    \"sy\":\"saya\", \"kmu\":\"kamu\", \"km\":\"kamu\", \"klu\":\"kalau\", \"kl\":\"kalau\", \"klo\":\"kalau\",\"klau\":\"kalau\",\n",
    "    \"f p\":\"free to play\",\"p w\":\"pay to win\",\"lmyan\":\"lumayan\",\"mayan\":\"lumayan\",\"w\":\"saya\",\n",
    "    \"mayanlah\":\"lumayanlah\", \"trlalu\":\"terlalu\",\"ak\":\"aku\",\"apk\":\"aplikasi\", \"pliss\":\"please\",\n",
    "    \"plisss\":\"please\",\"tlnk\":\"tolong\",\"tolonk\":\"tolong\", \"bikin\":\"buat\",\"emg\":\"memang\", \"gweh\":\"saya\",\n",
    "    \"emang\":\"memang\", \"manteb\":\"mantap\", \"yng\":\"yang\", \"kasi\":\"kasih\", \"liat\":\"lihat\", \"takkan\":\"tidak akan\",\n",
    "    \"pas\":\"ketika\", \"kalok\": \"kalau\",\"knapa\":\"kenapa\",\"donlot\": \"unduh\", \"knpa\":\"kenapa\",\"dah\":\"sudah\",\"dh\":\"sudah\",\n",
    "    \"capek\":\"lelah\",\"g\":\"tidak\", \"matiin\":\"mematikan\",\"thx\":\"thanks\", \"banyakin\": \"perbanyak\",\n",
    "    \"gatau\":\"tidak tahu\", \"gk tau\":\"tidak tahu\", \"gk tahu\":\"tidak tahu\", \"ngapain\":\"mengapa\",\n",
    "    \"sangt\":\"sangat\", \"gud\":\"good\", \"nyari\": \"mencari\", \"nyampe\": \"sampai\", \"ngelewatin\": \"melewati\",\n",
    "    \"ku kira\":\"kukira\",\"ampe\":\"sampai\",\"tambahin\":\"tambahkan\",\"buatin\":\"buatkan\", \"gampang\":\"mudah\",\n",
    "    \"gampangin\":\"permudah\",\"gampangan\":\"kemudahan\",\"bosenin\":\"membuat bosan\",\"bosenan\":\"kebosanan\",\n",
    "    \"ngelakuin\":\"melakukan\",\"ngelanjutin\":\"melanjutkan\",\"lanjutin\":\"melanjutkan\",\"ngatasin\":\"mengatasi\",\n",
    "    \"dapetin\":\"mendapatkan\", \"storynya\":\"cerita nya\", \"mantab\":\"mantap\", \"mulu\":\"terus\",\n",
    "}\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Stopword list gabungan\n",
    "# ----------------------------------------------\n",
    "stop_words_id = set(stopwords.words('indonesian'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "additional_stopwords = { \n",
    "    \"game\", \"nya\", \"sih\", \"aja\", \"yang\", \"gw\", \"gua\", \"gue\", \"kak\", \"min\", \"makasih\", \n",
    "    \"terimakasih\", \"keren\", \"ok\", \"tolong\", \"mohon\", \"banget\", \"si\", \"bagus\", \"keren\", \n",
    "    \"mantap\", \"gamenya\", \"okelah\", \"thanks\", \"good\", \"nice\", \"best\", \"seru\", \"oke\", \n",
    "    \"terima\", \"kasih\", \"semangat\", \"moga\", \"thanks\", \"thank\", \"you\", \"please\", \"main\", \n",
    "    \"baik\", \"ya\", \"sehat\", \"hoyoverse\", \"coy\", \"love\", \"ni\", \"suck\", \"like\", \"make\", \"play\"                  \n",
    "}\n",
    "stop_words = stop_words_id.union(stop_words_en).union(additional_stopwords)\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Setup stemmer\n",
    "# ----------------------------------------------\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Fungsi preprocessing per langkah\n",
    "# ----------------------------------------------\n",
    "def case_folding(text: str) -> str:\n",
    "    return str(text).lower()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    words = text.split()\n",
    "    words = [normalization_dict.get(w, w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return re.findall(r'\\b[^\\d\\W_]+\\b', text)\n",
    "\n",
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    return [stemmer.stem(t) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95554d1-3f23-450f-96eb-20ae3f30e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# CELL 2: FUNGSI PREPROCESS UNTUK 1 FILE\n",
    "# ==============================================\n",
    "def preprocess_file(input_csv: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Menerapkan pipeline preprocessing (case folding, cleaning,\n",
    "    normalisasi, tokenizing, stopword removal, stemming)\n",
    "    ke satu file CSV dan menyimpan hasil ke output_csv.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== PREPROCESSING: {input_csv} ===\")\n",
    "    if not os.path.exists(input_csv):\n",
    "        raise FileNotFoundError(f\"File tidak ditemukan: {input_csv}\")\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Deteksi kolom teks utama\n",
    "    possible_text_cols = ['review', 'content', 'comment', 'text', 'body', 'reviewBody']\n",
    "    TEXT_COL = next((c for c in possible_text_cols if c in df.columns), None)\n",
    "    if TEXT_COL is None:\n",
    "        raise ValueError(f\"Tidak ditemukan kolom teks utama di {input_csv}. \"\n",
    "                         f\"Kolom yang dicek: {possible_text_cols}\")\n",
    "    \n",
    "    # Ambil hanya 1 kolom teks dan buang NaN\n",
    "    df = df[[TEXT_COL]].rename(columns={TEXT_COL: 'content'}).dropna().reset_index(drop=True)\n",
    "    print(f\"  Data dimuat: {len(df)} baris dari kolom '{TEXT_COL}' -> 'content'\")\n",
    "    \n",
    "    # Jalankan pipeline bertahap\n",
    "    df[\"case_folding\"] = df[\"content\"].apply(case_folding)\n",
    "    df[\"cleaned\"] = df[\"case_folding\"].apply(clean_text)\n",
    "    df[\"normalized\"] = df[\"cleaned\"].apply(normalize_text)\n",
    "    df[\"token\"] = df[\"normalized\"].apply(tokenize_text)\n",
    "    df[\"no_stopword\"] = df[\"token\"].apply(remove_stopwords)\n",
    "    df[\"stemmed\"] = df[\"no_stopword\"].apply(stem_tokens)\n",
    "    df[\"clean_final\"] = df[\"stemmed\"].apply(lambda x: \" \".join(x))\n",
    "    \n",
    "    # Filter dokumen benar-benar kosong setelah preprocessing\n",
    "    before = len(df)\n",
    "    df[\"clean_final_length\"] = df[\"clean_final\"].str.split().apply(lambda x: len(x) if isinstance(x, list) else len(str(x).split()))\n",
    "    df = df[df[\"clean_final\"].str.strip() != \"\"].copy()\n",
    "    after = len(df)\n",
    "    df = df.drop(columns=[\"clean_final_length\"])\n",
    "    \n",
    "    print(f\"  Setelah filter kosong: {after} dokumen tersisa (buang {before - after})\")\n",
    "    \n",
    "    # Simpan hasil (minimal kolom yang nanti dipakai BERTopic: content, clean_final)\n",
    "    out_cols = [\"content\", \"clean_final\"]\n",
    "    df[out_cols].to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"  Hasil disimpan ke: {output_csv}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74970ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menyiapkan model embedding IndoBERT...\n",
      "  Menggunakan model lokal: ./model/indobert-base-p1\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# CELL 3: SETUP BERTOPIC PIPELINE UNTUK BANYAK DATASET\n",
    "# ==============================================\n",
    "import time\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import Counter\n",
    "\n",
    "EMB_CACHE_DIR = \"emb_cache\"\n",
    "os.makedirs(EMB_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "LOCAL_INDOBERT_PATH = \"./model/indobert-base-p1\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"Menyiapkan model embedding IndoBERT...\")\n",
    "if os.path.exists(LOCAL_INDOBERT_PATH):\n",
    "    print(f\"  Menggunakan model lokal: {LOCAL_INDOBERT_PATH}\")\n",
    "    embedding_model = SentenceTransformer(LOCAL_INDOBERT_PATH)\n",
    "else:\n",
    "    print(\"  Mengunduh: indobenchmark/indobert-base-p1\")\n",
    "    embedding_model = SentenceTransformer(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "def auto_param_rules(n_docs: int):\n",
    "    \"\"\"\n",
    "    Param otomatis bergantung ukuran dataset.\n",
    "    Sama seperti pipeline utama: fokus topik cukup, outlier sedang.\n",
    "    \"\"\"\n",
    "    if n_docs < 3000:\n",
    "        min_df = 2\n",
    "        max_df = 0.95\n",
    "        n_neighbors = 15\n",
    "        n_components = 5\n",
    "        min_cluster_size = max(5, int(0.01 * n_docs))\n",
    "        min_samples = max(5, int(min_cluster_size / 3))\n",
    "    elif n_docs < 7000:\n",
    "        min_df = max(2, int(0.002 * n_docs))\n",
    "        max_df = 0.9\n",
    "        n_neighbors = 20\n",
    "        n_components = 5\n",
    "        min_cluster_size = max(10, int(0.006 * n_docs))\n",
    "        min_samples = max(5, int(min_cluster_size / 3))\n",
    "    else:\n",
    "        min_df = 0.0015\n",
    "        max_df = 0.8\n",
    "        n_neighbors = 35\n",
    "        n_components = 5\n",
    "        min_cluster_size = max(45, int(0.003 * n_docs))\n",
    "        min_samples = max(10, int(min_cluster_size / 3))\n",
    "    return {\n",
    "        \"min_df\": min_df,\n",
    "        \"max_df\": max_df,\n",
    "        \"n_neighbors\": n_neighbors,\n",
    "        \"n_components\": n_components,\n",
    "        \"min_cluster_size\": min_cluster_size,\n",
    "        \"min_samples\": min_samples\n",
    "    }\n",
    "\n",
    "def run_bertopic_for_preprocessed(label: str, preprocessed_csv: str, output_root: str = \"Hasil_loop\"):\n",
    "    \"\"\"\n",
    "    Menjalankan pipeline BERTopic untuk 1 dataset yang SUDAH dipreprocessing\n",
    "    (harus punya kolom 'clean_final').\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"RUN BERTOPIC DATASET: {label}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(preprocessed_csv):\n",
    "        print(f\"[SKIP] File tidak ditemukan: {preprocessed_csv}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(preprocessed_csv, dtype=str)\n",
    "    if \"clean_final\" not in df.columns:\n",
    "        raise ValueError(f\"Kolom 'clean_final' tidak ditemukan di {preprocessed_csv}\")\n",
    "    \n",
    "    docs = df[\"clean_final\"].astype(str).tolist()\n",
    "    N = len(docs)\n",
    "    print(f\"  Jumlah dokumen: {N}\")\n",
    "    \n",
    "    params = auto_param_rules(N)\n",
    "    print(\"  Parameter otomatis:\")\n",
    "    for k, v in params.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "    \n",
    "    # Embeddings cache spesifik per (label, N)\n",
    "    emb_cache_path = os.path.join(EMB_CACHE_DIR, f\"embeddings_{N}.npy\")\n",
    "    if os.path.exists(emb_cache_path):\n",
    "        embeddings = np.load(emb_cache_path)\n",
    "        print(f\"  Embeddings dimuat dari cache: {emb_cache_path}\")\n",
    "    else:\n",
    "        print(\"  Menghitung embeddings...\")\n",
    "        embeddings = embedding_model.encode(docs, batch_size=BATCH_SIZE, show_progress_bar=True)\n",
    "        np.save(emb_cache_path, embeddings)\n",
    "        print(f\"  Embeddings disimpan ke: {emb_cache_path}\")\n",
    "    \n",
    "    # Vectorizer\n",
    "    min_df = params[\"min_df\"]\n",
    "    max_df = params[\"max_df\"]\n",
    "    if isinstance(min_df, float):\n",
    "        vectorizer = CountVectorizer(min_df=min_df, max_df=max_df, ngram_range=(1,2))\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(min_df=int(min_df), max_df=max_df, ngram_range=(1,2))\n",
    "    \n",
    "    vectorizer.fit(docs)\n",
    "    vocab_size = len(vectorizer.get_feature_names_out())\n",
    "    print(f\"  Vocabulary size: {vocab_size}\")\n",
    "    if vocab_size == 0:\n",
    "        print(\"  [STOP] Vocabulary kosong. Sesuaikan min_df/ngram_range.\")\n",
    "        return None\n",
    "    \n",
    "    # UMAP & HDBSCAN\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=params[\"n_neighbors\"],\n",
    "        n_components=params[\"n_components\"],\n",
    "        min_dist=0.05,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    hdbscan_model = HDBSCAN(\n",
    "        min_cluster_size=params[\"min_cluster_size\"],\n",
    "        min_samples=params[\"min_samples\"],\n",
    "        metric=\"euclidean\",\n",
    "        prediction_data=True,\n",
    "        cluster_selection_method=\"leaf\"\n",
    "    )\n",
    "    \n",
    "    # BERTopic\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer,\n",
    "        language=\"multilingual\",\n",
    "        calculate_probabilities=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    start = time.time()\n",
    "    topics, probs = topic_model.fit_transform(docs, embeddings=embeddings)\n",
    "    elapsed = (time.time() - start) / 60.0\n",
    "    print(f\"  Waktu fit: {elapsed:.2f} menit\")\n",
    "    \n",
    "    # Ringkasan\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    num_topics = len(topic_info[topic_info[\"Topic\"] != -1])\n",
    "    outlier_ratio = (np.array(topics) == -1).sum() / len(topics)\n",
    "    print(f\"  Topik efektif: {num_topics}\")\n",
    "    print(f\"  Outlier ratio: {outlier_ratio:.2%}\")\n",
    "    \n",
    "    counts = Counter(topics)\n",
    "    non_out_counts = {t: c for t, c in counts.items() if t != -1}\n",
    "    if non_out_counts:\n",
    "        total_non_out = sum(non_out_counts.values())\n",
    "        top_pairs = sorted(non_out_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"  5 topik terbesar (non-outlier):\")\n",
    "        for tid, c in top_pairs:\n",
    "            print(f\"    Topic {tid}: {c} dokumen ({c/total_non_out*100:.2f}%)\")\n",
    "    \n",
    "    # Coherence\n",
    "    tokenized_docs = [d.split() for d in docs]\n",
    "    dictionary = Dictionary(tokenized_docs)\n",
    "    coh_scores = []\n",
    "    for tid in topic_info[topic_info[\"Topic\"] != -1][\"Topic\"]:\n",
    "        tid = int(tid)\n",
    "        count = int(topic_info.loc[topic_info[\"Topic\"] == tid, \"Count\"].values[0])\n",
    "        if count < max(5, int(0.002 * N)):\n",
    "            continue\n",
    "        words = [w for w, _ in topic_model.get_topic(tid)]\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            cm = CoherenceModel(\n",
    "                topics=[words],\n",
    "                texts=tokenized_docs,\n",
    "                dictionary=dictionary,\n",
    "                coherence=\"c_v\"\n",
    "            )\n",
    "            coh_scores.append(cm.get_coherence())\n",
    "        except:\n",
    "            pass\n",
    "    mean_coh = float(np.mean(coh_scores)) if coh_scores else float(\"nan\")\n",
    "    print(f\"  Mean coherence (c_v): {mean_coh:.4f}\")\n",
    "    \n",
    "    # Simpan output\n",
    "    out_dir = os.path.join(output_root, label)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    topic_info.to_csv(os.path.join(out_dir, \"topic_info.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    df_out = df.copy()\n",
    "    df_out[\"pred_topic\"] = topics\n",
    "    df_out.to_csv(os.path.join(out_dir, \"docs_with_topic.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # evaluasi per topik yang sudah punya skor\n",
    "    eval_rows = []\n",
    "    for tid, coh in zip(topic_info[topic_info[\"Topic\"] != -1][\"Topic\"], coh_scores):\n",
    "        eval_rows.append({\"Topic_ID\": int(tid), \"Coherence_c_v\": coh})\n",
    "    if eval_rows:\n",
    "        pd.DataFrame(eval_rows).to_csv(os.path.join(out_dir, \"evaluasi_per_topik_c_v.csv\"),\n",
    "                                       index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"  Output disimpan di folder: {out_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"N_docs\": N,\n",
    "        \"num_topics\": num_topics,\n",
    "        \"outlier_ratio\": outlier_ratio,\n",
    "        \"mean_coherence_c_v\": mean_coh\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447ce84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPROCESSING: honkai_dummy_5pct.csv ===\n",
      "  Data dimuat: 1270 baris dari kolom 'content' -> 'content'\n",
      "  Setelah filter kosong: 1122 dokumen tersisa (buang 148)\n",
      "  Hasil disimpan ke: dummy_preprocessed_5pct.csv\n",
      "\n",
      "=== PREPROCESSING: honkai_dummy_10pct.csv ===\n",
      "  Data dimuat: 2541 baris dari kolom 'content' -> 'content'\n",
      "  Setelah filter kosong: 2225 dokumen tersisa (buang 316)\n",
      "  Hasil disimpan ke: dummy_preprocessed_10pct.csv\n",
      "\n",
      "=== PREPROCESSING: honkai_dummy_25pct.csv ===\n",
      "  Data dimuat: 6353 baris dari kolom 'content' -> 'content'\n",
      "  Setelah filter kosong: 5560 dokumen tersisa (buang 793)\n",
      "  Hasil disimpan ke: dummy_preprocessed_25pct.csv\n",
      "\n",
      "======================================================================\n",
      "RUN BERTOPIC DATASET: dummy_5pct\n",
      "======================================================================\n",
      "  Jumlah dokumen: 1122\n",
      "  Parameter otomatis:\n",
      "    min_df: 2\n",
      "    max_df: 0.95\n",
      "    n_neighbors: 15\n",
      "    n_components: 5\n",
      "    min_cluster_size: 11\n",
      "    min_samples: 5\n",
      "  Embeddings dimuat dari cache: emb_cache\\embeddings_1122.npy\n",
      "  Vocabulary size: 1221\n",
      "  Waktu fit: 0.60 menit\n",
      "  Topik efektif: 28\n",
      "  Outlier ratio: 34.14%\n",
      "  5 topik terbesar (non-outlier):\n",
      "    Topic 0: 74 dokumen (10.01%)\n",
      "    Topic 1: 68 dokumen (9.20%)\n",
      "    Topic 2: 47 dokumen (6.36%)\n",
      "    Topic 3: 44 dokumen (5.95%)\n",
      "    Topic 4: 42 dokumen (5.68%)\n",
      "  Mean coherence (c_v): 0.4449\n",
      "  Output disimpan di folder: Hasil_loop\\dummy_5pct\n",
      "\n",
      "======================================================================\n",
      "RUN BERTOPIC DATASET: dummy_10pct\n",
      "======================================================================\n",
      "  Jumlah dokumen: 2225\n",
      "  Parameter otomatis:\n",
      "    min_df: 2\n",
      "    max_df: 0.95\n",
      "    n_neighbors: 15\n",
      "    n_components: 5\n",
      "    min_cluster_size: 22\n",
      "    min_samples: 7\n",
      "  Embeddings dimuat dari cache: emb_cache\\embeddings_2225.npy\n",
      "  Vocabulary size: 2267\n",
      "  Waktu fit: 1.08 menit\n",
      "  Topik efektif: 30\n",
      "  Outlier ratio: 42.34%\n",
      "  5 topik terbesar (non-outlier):\n",
      "    Topic 0: 100 dokumen (7.79%)\n",
      "    Topic 1: 92 dokumen (7.17%)\n",
      "    Topic 2: 70 dokumen (5.46%)\n",
      "    Topic 3: 68 dokumen (5.30%)\n",
      "    Topic 4: 55 dokumen (4.29%)\n",
      "  Mean coherence (c_v): 0.5080\n",
      "  Output disimpan di folder: Hasil_loop\\dummy_10pct\n",
      "\n",
      "======================================================================\n",
      "RUN BERTOPIC DATASET: dummy_25pct\n",
      "======================================================================\n",
      "  Jumlah dokumen: 5560\n",
      "  Parameter otomatis:\n",
      "    min_df: 11\n",
      "    max_df: 0.9\n",
      "    n_neighbors: 20\n",
      "    n_components: 5\n",
      "    min_cluster_size: 33\n",
      "    min_samples: 11\n",
      "  Embeddings dimuat dari cache: emb_cache\\embeddings_5560.npy\n",
      "  Vocabulary size: 653\n",
      "  Waktu fit: 2.86 menit\n",
      "  Topik efektif: 33\n",
      "  Outlier ratio: 52.12%\n",
      "  5 topik terbesar (non-outlier):\n",
      "    Topic 0: 313 dokumen (11.76%)\n",
      "    Topic 1: 233 dokumen (8.75%)\n",
      "    Topic 2: 231 dokumen (8.68%)\n",
      "    Topic 3: 124 dokumen (4.66%)\n",
      "    Topic 4: 105 dokumen (3.94%)\n",
      "  Mean coherence (c_v): 0.4943\n",
      "  Output disimpan di folder: Hasil_loop\\dummy_25pct\n",
      "\n",
      "================= RINGKASAN SEMUA DATASET =================\n",
      "         label  N_docs  num_topics  outlier_ratio  mean_coherence_c_v\n",
      "0   dummy_5pct    1122          28       0.341355            0.444881\n",
      "1  dummy_10pct    2225          30       0.423371            0.507977\n",
      "2  dummy_25pct    5560          33       0.521223            0.494310\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# CELL 4: JALANKAN PREPROCESSING + BERTOPIC\n",
    "# ==============================================\n",
    "\n",
    "# 1) PREPROCESSING (kalau dummy masih raw)\n",
    "# Kalau dummy kamu sudah dalam bentuk \"dummy_preprocessed_5pct.csv\" dkk yang SAMA\n",
    "# formatnya dengan hasil_preprocessing (punya clean_final), kamu bisa SKIP blok ini\n",
    "# dan langsung ke langkah run_bertopic_for_preprocessed di bawah.\n",
    "\n",
    "# Contoh (ganti nama file sesuaikan):\n",
    "# preprocess_file(\"scrapped_data_honkai_star_rail.csv\", \"hasil_preprocessing.csv\")\n",
    "preprocess_file(\"honkai_dummy_5pct.csv\", \"dummy_preprocessed_5pct.csv\")\n",
    "preprocess_file(\"honkai_dummy_10pct.csv\", \"dummy_preprocessed_10pct.csv\")\n",
    "preprocess_file(\"honkai_dummy_25pct.csv\", \"dummy_preprocessed_25pct.csv\")\n",
    "\n",
    "# 2) LOOP BERTOPIC UNTUK DATASET FULL + DUMMY\n",
    "datasets_preprocessed = [\n",
    "    # {\"label\": \"full\",        \"path\": \"hasil_preprocessing.csv\"},\n",
    "    {\"label\": \"dummy_5pct\",  \"path\": \"dummy_preprocessed_5pct.csv\"},\n",
    "    {\"label\": \"dummy_10pct\", \"path\": \"dummy_preprocessed_10pct.csv\"},\n",
    "    {\"label\": \"dummy_25pct\", \"path\": \"dummy_preprocessed_25pct.csv\"},\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for d in datasets_preprocessed:\n",
    "    res = run_bertopic_for_preprocessed(d[\"label\"], d[\"path\"], output_root=\"Hasil_loop\")\n",
    "    if res is not None:\n",
    "        all_results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\n================= RINGKASAN SEMUA DATASET =================\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
